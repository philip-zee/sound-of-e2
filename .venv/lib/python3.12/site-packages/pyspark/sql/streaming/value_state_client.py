#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from typing import Union, Tuple, Optional

from pyspark.sql.streaming.stateful_processor_api_client import StatefulProcessorApiClient, ValueStateWriteBufferEntry
from pyspark.sql.types import StructType
from pyspark.errors import PySparkRuntimeError

__all__ = ["ValueStateClient"]


class ValueStateClient:
    def __init__(
        self,
        stateful_processor_api_client: StatefulProcessorApiClient,
        schema: Union[StructType, str],
        is_ttl_state: bool = False, # EDGE
    ) -> None:
        self._stateful_processor_api_client = stateful_processor_api_client
        if isinstance(schema, str):
            self.schema = self._stateful_processor_api_client._parse_string_schema(schema)
        else:
            self.schema = schema
        # BEGIN-EDGE
        self.worker_cache_enabled = (
            self._stateful_processor_api_client.worker_cache_enabled
            and not is_ttl_state and self._stateful_processor_api_client.is_realtime_mode
        )
        # END_EDGE

    def exists(self, state_name: str) -> bool:
        import pyspark.sql.streaming.proto.StateMessage_pb2 as stateMessage
        # BEGIN-EDGE
        if self.worker_cache_enabled:
            cache_result = self._exists_with_worker_cache(state_name)
            if cache_result is not None:
                return cache_result
        # END-EDGE

        exists_call = stateMessage.Exists()
        value_state_call = stateMessage.ValueStateCall(stateName=state_name, exists=exists_call)
        state_variable_request = stateMessage.StateVariableRequest(valueStateCall=value_state_call)
        message = stateMessage.StateRequest(stateVariableRequest=state_variable_request)

        self._stateful_processor_api_client._send_proto_message(message.SerializeToString())
        response_message = self._stateful_processor_api_client._receive_proto_message()
        status = response_message[0]
        if status == 0:
            return True
        elif status == 2:
            # Expect status code is 2 when state variable doesn't have a value.
            return False
        else:
            # TODO(SPARK-49233): Classify user facing errors.
            raise PySparkRuntimeError(
                f"Error checking value state exists: " f"{response_message[1]}"
            )

    def get(self, state_name: str) -> Optional[Tuple]:
        import pyspark.sql.streaming.proto.StateMessage_pb2 as stateMessage
        # BEGIN-EDGE
        if self.worker_cache_enabled:
            cache_result = self._get_with_worker_cache(state_name)
            if cache_result is not None and cache_result[1]:
                # cache_result is a (value, found_in_cache) tuple, only returns value if
                # found_in_cache is True.
                return cache_result[0]
        # END-EDGE

        get_call = stateMessage.Get()
        value_state_call = stateMessage.ValueStateCall(stateName=state_name, get=get_call)
        state_variable_request = stateMessage.StateVariableRequest(valueStateCall=value_state_call)
        message = stateMessage.StateRequest(stateVariableRequest=state_variable_request)

        self._stateful_processor_api_client._send_proto_message(message.SerializeToString())
        response_message = self._stateful_processor_api_client._receive_proto_message()
        status = response_message[0]
        if status == 0:
            if len(response_message[2]) == 0:
                return None
            data = self._stateful_processor_api_client._deserialize_from_bytes(response_message[2])
            return tuple(data)
        else:
            # TODO(SPARK-49233): Classify user facing errors.
            raise PySparkRuntimeError(f"Error getting value state: " f"{response_message[1]}")

    def update(self, state_name: str, value: Tuple) -> None:
        import pyspark.sql.streaming.proto.StateMessage_pb2 as stateMessage
        # BEGIN-EDGE
        if self.worker_cache_enabled:
            return self._update_with_worker_cache(state_name, value)
        # END-EDGE

        bytes = self._stateful_processor_api_client._serialize_to_bytes(self.schema, value)
        update_call = stateMessage.ValueStateUpdate(value=bytes)
        value_state_call = stateMessage.ValueStateCall(
            stateName=state_name, valueStateUpdate=update_call
        )
        state_variable_request = stateMessage.StateVariableRequest(valueStateCall=value_state_call)
        message = stateMessage.StateRequest(stateVariableRequest=state_variable_request)

        self._stateful_processor_api_client._send_proto_message(message.SerializeToString())
        response_message = self._stateful_processor_api_client._receive_proto_message()
        status = response_message[0]
        if status != 0:
            # TODO(SPARK-49233): Classify user facing errors.
            raise PySparkRuntimeError(f"Error updating value state: " f"{response_message[1]}")

    def clear(self, state_name: str) -> None:
        import pyspark.sql.streaming.proto.StateMessage_pb2 as stateMessage
        # BEGIN-EDGE
        if self.worker_cache_enabled:
            return self._clear_with_worker_cache(state_name)
        # END-EDGE

        clear_call = stateMessage.Clear()
        value_state_call = stateMessage.ValueStateCall(stateName=state_name, clear=clear_call)
        state_variable_request = stateMessage.StateVariableRequest(valueStateCall=value_state_call)
        message = stateMessage.StateRequest(stateVariableRequest=state_variable_request)

        self._stateful_processor_api_client._send_proto_message(message.SerializeToString())
        response_message = self._stateful_processor_api_client._receive_proto_message()
        status = response_message[0]
        if status != 0:
            # TODO(SPARK-49233): Classify user facing errors.
            raise PySparkRuntimeError(f"Error clearing value state: " f"{response_message[1]}")

    # BEGIN-EDGE
    def _exists_with_worker_cache(self, state_name: str) -> Optional[bool]:
        """Handle exists() with worker cache."""
        if state_name in self._stateful_processor_api_client.value_state_write_buffer:
            if self._stateful_processor_api_client.value_state_write_buffer[state_name] is not None:
                return True
            else:
                return False
        return None  # Continue with original logic

    def _get_with_worker_cache(self, state_name: str) -> Optional[Tuple[Optional[Tuple], bool]]:
        """Handle get() with worker cache.
        Returns (value, found_in_cache) tuple or None to continue with original logic.
        """
        if state_name in self._stateful_processor_api_client.value_state_write_buffer:
            if self._stateful_processor_api_client.value_state_write_buffer[state_name] is None:
                # If the value state is cached as None, it means the state was cleared previously.
                return (None, True)
            else:
                # If the value state is cached, return it directly.
                value_state_entry = self._stateful_processor_api_client.value_state_write_buffer[
                    state_name
                ]
                return (value_state_entry.value, True)
        return None  # Continue with original logic

    def _update_with_worker_cache(self, state_name: str, value: Tuple) -> None:
        """Handle update() with worker cache."""
        self._stateful_processor_api_client.value_state_write_buffer[
            state_name
        ] = ValueStateWriteBufferEntry(value=value, schema=self.schema)

    def _clear_with_worker_cache(self, state_name: str) -> None:
        """Handle clear() with worker cache."""
        # Set the value of ValueState to None to represent clear() call.
        self._stateful_processor_api_client.value_state_write_buffer[state_name] = None
    # END-EDGE
