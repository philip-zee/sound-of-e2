#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file

DATABRICKS CONFIDENTIAL & PROPRIETARY
__________________

Copyright 2025-present Databricks, Inc.
All Rights Reserved.

NOTICE:  All information contained herein is, and remains the property of Databricks, Inc.
and its suppliers, if any.  The intellectual and technical concepts contained herein are
proprietary to Databricks, Inc. and its suppliers and may be covered by U.S. and foreign Patents,
patents in process, and are protected by trade secret and/or copyright law. Dissemination, use,
or reproduction of this information is strictly forbidden unless prior written permission is
obtained from Databricks, Inc.

If you view or obtain a copy of this information and believe Databricks, Inc. may not have
intended it to be made available, please promptly report it to Databricks Legal Department
@ legal@databricks.com.
"""
import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import pyspark.sql.connect.proto.common_pb2
import pyspark.sql.connect.proto.expressions_pb2
import pyspark.sql.connect.proto.pipelines_pb2
import sys
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _SCDType:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _SCDTypeEnumTypeWrapper(
    google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_SCDType.ValueType], builtins.type
):  # noqa: F821
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    SCD_TYPE_UNSPECIFIED: _SCDType.ValueType  # 0
    SCD_TYPE_1: _SCDType.ValueType  # 1
    SCD_TYPE_2: _SCDType.ValueType  # 2

class SCDType(_SCDType, metaclass=_SCDTypeEnumTypeWrapper): ...

SCD_TYPE_UNSPECIFIED: SCDType.ValueType  # 0
SCD_TYPE_1: SCDType.ValueType  # 1
SCD_TYPE_2: SCDType.ValueType  # 2
global___SCDType = SCDType

class _ExpectationViolationAction:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _ExpectationViolationActionEnumTypeWrapper(
    google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[
        _ExpectationViolationAction.ValueType
    ],
    builtins.type,
):  # noqa: F821
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    UNSPECIFIED: _ExpectationViolationAction.ValueType  # 0
    ALLOW: _ExpectationViolationAction.ValueType  # 1
    """Allow the row to pass through."""
    DROP: _ExpectationViolationAction.ValueType  # 2
    """Drop the row."""
    FAIL: _ExpectationViolationAction.ValueType  # 3
    """Fail the update."""

class ExpectationViolationAction(
    _ExpectationViolationAction, metaclass=_ExpectationViolationActionEnumTypeWrapper
):
    """The action to take when an expectation invariant is violated."""

UNSPECIFIED: ExpectationViolationAction.ValueType  # 0
ALLOW: ExpectationViolationAction.ValueType  # 1
"""Allow the row to pass through."""
DROP: ExpectationViolationAction.ValueType  # 2
"""Drop the row."""
FAIL: ExpectationViolationAction.ValueType  # 3
"""Fail the update."""
global___ExpectationViolationAction = ExpectationViolationAction

class AutoCdcFlowDetails(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    SOURCE_FIELD_NUMBER: builtins.int
    KEYS_FIELD_NUMBER: builtins.int
    SEQUENCE_BY_FIELD_NUMBER: builtins.int
    WHERE_FIELD_NUMBER: builtins.int
    IGNORE_NULL_UPDATES_FIELD_NUMBER: builtins.int
    APPLY_AS_DELETES_FIELD_NUMBER: builtins.int
    APPLY_AS_TRUNCATES_FIELD_NUMBER: builtins.int
    COLUMN_LIST_FIELD_NUMBER: builtins.int
    EXCEPT_COLUMN_LIST_FIELD_NUMBER: builtins.int
    STORED_AS_SCD_TYPE_FIELD_NUMBER: builtins.int
    TRACK_HISTORY_COLUMN_LIST_FIELD_NUMBER: builtins.int
    TRACK_HISTORY_EXCEPT_COLUMN_LIST_FIELD_NUMBER: builtins.int
    ONCE_FIELD_NUMBER: builtins.int
    IGNORE_NULL_UPDATES_COLUMN_LIST_FIELD_NUMBER: builtins.int
    IGNORE_NULL_UPDATES_EXCEPT_COLUMN_LIST_FIELD_NUMBER: builtins.int
    COLUMNS_TO_UPDATE_FIELD_NUMBER: builtins.int
    source: builtins.str
    """The name of the CDC source to stream from."""
    @property
    def keys(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Column(s) that uniquely identify a row in source and target data."""
    @property
    def sequence_by(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Expression to order the source data."""
    @property
    def where(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Optional condition applied to source and target for optimizations like partition pruning."""
    ignore_null_updates: builtins.bool
    """Whether to ignore null values in source data updates."""
    @property
    def apply_as_deletes(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Delete condition for the merged operation."""
    @property
    def apply_as_truncates(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Truncate condition for the merged operation."""
    @property
    def column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Columns included in the output table."""
    @property
    def except_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Columns excluded from the output table."""
    stored_as_scd_type: global___SCDType.ValueType
    """SCD Type for target table"""
    @property
    def track_history_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Columns tracked for change history."""
    @property
    def track_history_except_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Columns not tracked for change history."""
    once: builtins.bool
    """If true, this flow runs only once."""
    @property
    def ignore_null_updates_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Subset of columns to ignore null in updates."""
    @property
    def ignore_null_updates_except_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Subset of columns excluded from ignoring null in updates."""
    @property
    def columns_to_update(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Column indicating which user columns to update or ignore."""
    def __init__(
        self,
        *,
        source: builtins.str | None = ...,
        keys: collections.abc.Iterable[pyspark.sql.connect.proto.expressions_pb2.Expression]
        | None = ...,
        sequence_by: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
        where: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
        ignore_null_updates: builtins.bool | None = ...,
        apply_as_deletes: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
        apply_as_truncates: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
        column_list: collections.abc.Iterable[pyspark.sql.connect.proto.expressions_pb2.Expression]
        | None = ...,
        except_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        stored_as_scd_type: global___SCDType.ValueType = ...,
        track_history_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        track_history_except_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        once: builtins.bool | None = ...,
        ignore_null_updates_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        ignore_null_updates_except_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        columns_to_update: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_apply_as_deletes",
            b"_apply_as_deletes",
            "_apply_as_truncates",
            b"_apply_as_truncates",
            "_columns_to_update",
            b"_columns_to_update",
            "_ignore_null_updates",
            b"_ignore_null_updates",
            "_once",
            b"_once",
            "_sequence_by",
            b"_sequence_by",
            "_source",
            b"_source",
            "_where",
            b"_where",
            "apply_as_deletes",
            b"apply_as_deletes",
            "apply_as_truncates",
            b"apply_as_truncates",
            "columns_to_update",
            b"columns_to_update",
            "ignore_null_updates",
            b"ignore_null_updates",
            "once",
            b"once",
            "sequence_by",
            b"sequence_by",
            "source",
            b"source",
            "where",
            b"where",
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_apply_as_deletes",
            b"_apply_as_deletes",
            "_apply_as_truncates",
            b"_apply_as_truncates",
            "_columns_to_update",
            b"_columns_to_update",
            "_ignore_null_updates",
            b"_ignore_null_updates",
            "_once",
            b"_once",
            "_sequence_by",
            b"_sequence_by",
            "_source",
            b"_source",
            "_where",
            b"_where",
            "apply_as_deletes",
            b"apply_as_deletes",
            "apply_as_truncates",
            b"apply_as_truncates",
            "column_list",
            b"column_list",
            "columns_to_update",
            b"columns_to_update",
            "except_column_list",
            b"except_column_list",
            "ignore_null_updates",
            b"ignore_null_updates",
            "ignore_null_updates_column_list",
            b"ignore_null_updates_column_list",
            "ignore_null_updates_except_column_list",
            b"ignore_null_updates_except_column_list",
            "keys",
            b"keys",
            "once",
            b"once",
            "sequence_by",
            b"sequence_by",
            "source",
            b"source",
            "stored_as_scd_type",
            b"stored_as_scd_type",
            "track_history_column_list",
            b"track_history_column_list",
            "track_history_except_column_list",
            b"track_history_except_column_list",
            "where",
            b"where",
        ],
    ) -> None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_apply_as_deletes", b"_apply_as_deletes"]
    ) -> typing_extensions.Literal["apply_as_deletes"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_apply_as_truncates", b"_apply_as_truncates"]
    ) -> typing_extensions.Literal["apply_as_truncates"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_columns_to_update", b"_columns_to_update"]
    ) -> typing_extensions.Literal["columns_to_update"] | None: ...
    @typing.overload
    def WhichOneof(
        self,
        oneof_group: typing_extensions.Literal["_ignore_null_updates", b"_ignore_null_updates"],
    ) -> typing_extensions.Literal["ignore_null_updates"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_once", b"_once"]
    ) -> typing_extensions.Literal["once"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_sequence_by", b"_sequence_by"]
    ) -> typing_extensions.Literal["sequence_by"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_source", b"_source"]
    ) -> typing_extensions.Literal["source"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_where", b"_where"]
    ) -> typing_extensions.Literal["where"] | None: ...

global___AutoCdcFlowDetails = AutoCdcFlowDetails

class EdgeTableDetails(google.protobuf.message.Message):
    """LDP tables supports several fields that are not supported by OSS Spark pipelines. These are
    captured here. This message is meant to be used in the extension field of
    PipelineCommand.DefineOutput.details.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class SqlConfEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: builtins.str
        value: builtins.str
        def __init__(
            self,
            *,
            key: builtins.str = ...,
            value: builtins.str = ...,
        ) -> None: ...
        def ClearField(
            self, field_name: typing_extensions.Literal["key", b"key", "value", b"value"]
        ) -> None: ...

    TABLE_DETAILS_FIELD_NUMBER: builtins.int
    CLUSTER_BY_AUTO_FIELD_NUMBER: builtins.int
    ROW_FILTER_FIELD_NUMBER: builtins.int
    SQL_CONF_FIELD_NUMBER: builtins.int
    PRIVATE_FIELD_NUMBER: builtins.int
    PATH_FIELD_NUMBER: builtins.int
    @property
    def table_details(
        self,
    ) -> pyspark.sql.connect.proto.pipelines_pb2.PipelineCommand.DefineOutput.TableDetails: ...
    cluster_by_auto: builtins.bool
    """Whether clustering columns should be auto selected."""
    row_filter: builtins.str
    """A row filter SQL clause that filters the rows in the table."""
    @property
    def sql_conf(self) -> google.protobuf.internal.containers.ScalarMap[builtins.str, builtins.str]:
        """SQL configurations to apply to all flows that target the table."""
    private: builtins.bool
    """Specifies that this table should be private. For UC pipelines, private tables
    are created with a disambiguated name and are hidden from users. For HMS pipelines, they are
    excluded from being added to the metastore
    """
    path: builtins.str
    """The path to the table."""
    def __init__(
        self,
        *,
        table_details: pyspark.sql.connect.proto.pipelines_pb2.PipelineCommand.DefineOutput.TableDetails
        | None = ...,
        cluster_by_auto: builtins.bool | None = ...,
        row_filter: builtins.str | None = ...,
        sql_conf: collections.abc.Mapping[builtins.str, builtins.str] | None = ...,
        private: builtins.bool | None = ...,
        path: builtins.str | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_cluster_by_auto",
            b"_cluster_by_auto",
            "_path",
            b"_path",
            "_private",
            b"_private",
            "_row_filter",
            b"_row_filter",
            "_table_details",
            b"_table_details",
            "cluster_by_auto",
            b"cluster_by_auto",
            "path",
            b"path",
            "private",
            b"private",
            "row_filter",
            b"row_filter",
            "table_details",
            b"table_details",
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_cluster_by_auto",
            b"_cluster_by_auto",
            "_path",
            b"_path",
            "_private",
            b"_private",
            "_row_filter",
            b"_row_filter",
            "_table_details",
            b"_table_details",
            "cluster_by_auto",
            b"cluster_by_auto",
            "path",
            b"path",
            "private",
            b"private",
            "row_filter",
            b"row_filter",
            "sql_conf",
            b"sql_conf",
            "table_details",
            b"table_details",
        ],
    ) -> None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_cluster_by_auto", b"_cluster_by_auto"]
    ) -> typing_extensions.Literal["cluster_by_auto"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_path", b"_path"]
    ) -> typing_extensions.Literal["path"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_private", b"_private"]
    ) -> typing_extensions.Literal["private"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_row_filter", b"_row_filter"]
    ) -> typing_extensions.Literal["row_filter"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_table_details", b"_table_details"]
    ) -> typing_extensions.Literal["table_details"] | None: ...

global___EdgeTableDetails = EdgeTableDetails

class EdgeSourceCodeLocationDetails(google.protobuf.message.Message):
    """LDP source code locations supports fields that are not supported by OSS Spark Pipelines.
    These are captured here. This message is meant to be used in the extension field of
    SourceCodeLocation
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    CELL_NUMBER_FIELD_NUMBER: builtins.int
    cell_number: builtins.int
    """The notebook cell number this pipeline source code was defined in."""
    def __init__(
        self,
        *,
        cell_number: builtins.int | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_cell_number", b"_cell_number", "cell_number", b"cell_number"
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_cell_number", b"_cell_number", "cell_number", b"cell_number"
        ],
    ) -> None: ...
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_cell_number", b"_cell_number"]
    ) -> typing_extensions.Literal["cell_number"] | None: ...

global___EdgeSourceCodeLocationDetails = EdgeSourceCodeLocationDetails

class EdgeWriteRelationFlowDetails(google.protobuf.message.Message):
    """LDP flows supports several fields that are not supported by OSS Spark pipelines. These are
    captured here. This message is meant to be used in the extension field of
    PipelineCommand.DefineFlow.details.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    RELATION_FLOW_DETAILS_FIELD_NUMBER: builtins.int
    OUTPUT_MODE_FIELD_NUMBER: builtins.int
    COMMENT_FIELD_NUMBER: builtins.int
    @property
    def relation_flow_details(
        self,
    ) -> (
        pyspark.sql.connect.proto.pipelines_pb2.PipelineCommand.DefineFlow.WriteRelationFlowDetails
    ): ...
    output_mode: builtins.str
    """The output mode for the flow."""
    comment: builtins.str
    """An optional comment for the flow."""
    def __init__(
        self,
        *,
        relation_flow_details: pyspark.sql.connect.proto.pipelines_pb2.PipelineCommand.DefineFlow.WriteRelationFlowDetails
        | None = ...,
        output_mode: builtins.str | None = ...,
        comment: builtins.str | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_comment",
            b"_comment",
            "_output_mode",
            b"_output_mode",
            "_relation_flow_details",
            b"_relation_flow_details",
            "comment",
            b"comment",
            "output_mode",
            b"output_mode",
            "relation_flow_details",
            b"relation_flow_details",
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_comment",
            b"_comment",
            "_output_mode",
            b"_output_mode",
            "_relation_flow_details",
            b"_relation_flow_details",
            "comment",
            b"comment",
            "output_mode",
            b"output_mode",
            "relation_flow_details",
            b"relation_flow_details",
        ],
    ) -> None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_comment", b"_comment"]
    ) -> typing_extensions.Literal["comment"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_output_mode", b"_output_mode"]
    ) -> typing_extensions.Literal["output_mode"] | None: ...
    @typing.overload
    def WhichOneof(
        self,
        oneof_group: typing_extensions.Literal["_relation_flow_details", b"_relation_flow_details"],
    ) -> typing_extensions.Literal["relation_flow_details"] | None: ...

global___EdgeWriteRelationFlowDetails = EdgeWriteRelationFlowDetails

class DeprecatedAPIUsagePipelineCommand(google.protobuf.message.Message):
    """Records that a deprecated API was used in a pipeline definition file.
    Intended for use inside the extension field of PipelineCommand.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATAFLOW_GRAPH_ID_FIELD_NUMBER: builtins.int
    DEPRECATED_FEATURE_FIELD_NUMBER: builtins.int
    MESSAGE_FIELD_NUMBER: builtins.int
    SOURCE_CODE_LOCATION_FIELD_NUMBER: builtins.int
    dataflow_graph_id: builtins.str
    """The graph being defined when the API usage occurred."""
    deprecated_feature: builtins.str
    """The feature corresponding to the deprecated API usage. A value from the
    pipelines feature proto enum.
    """
    message: builtins.str
    """A human-readable message describing the usage."""
    @property
    def source_code_location(self) -> pyspark.sql.connect.proto.pipelines_pb2.SourceCodeLocation:
        """The source code location where the deprecated API was used."""
    def __init__(
        self,
        *,
        dataflow_graph_id: builtins.str | None = ...,
        deprecated_feature: builtins.str | None = ...,
        message: builtins.str | None = ...,
        source_code_location: pyspark.sql.connect.proto.pipelines_pb2.SourceCodeLocation
        | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_dataflow_graph_id",
            b"_dataflow_graph_id",
            "_deprecated_feature",
            b"_deprecated_feature",
            "_message",
            b"_message",
            "_source_code_location",
            b"_source_code_location",
            "dataflow_graph_id",
            b"dataflow_graph_id",
            "deprecated_feature",
            b"deprecated_feature",
            "message",
            b"message",
            "source_code_location",
            b"source_code_location",
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_dataflow_graph_id",
            b"_dataflow_graph_id",
            "_deprecated_feature",
            b"_deprecated_feature",
            "_message",
            b"_message",
            "_source_code_location",
            b"_source_code_location",
            "dataflow_graph_id",
            b"dataflow_graph_id",
            "deprecated_feature",
            b"deprecated_feature",
            "message",
            b"message",
            "source_code_location",
            b"source_code_location",
        ],
    ) -> None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_dataflow_graph_id", b"_dataflow_graph_id"]
    ) -> typing_extensions.Literal["dataflow_graph_id"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_deprecated_feature", b"_deprecated_feature"]
    ) -> typing_extensions.Literal["deprecated_feature"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_message", b"_message"]
    ) -> typing_extensions.Literal["message"] | None: ...
    @typing.overload
    def WhichOneof(
        self,
        oneof_group: typing_extensions.Literal["_source_code_location", b"_source_code_location"],
    ) -> typing_extensions.Literal["source_code_location"] | None: ...

global___DeprecatedAPIUsagePipelineCommand = DeprecatedAPIUsagePipelineCommand

class UnsupportedAPIUsagePipelineCommand(google.protobuf.message.Message):
    """Records that an unsupported Python API was used in a pipeline definition file.
    Intended for use inside the extension field of PipelineCommand.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATAFLOW_GRAPH_ID_FIELD_NUMBER: builtins.int
    OPERATION_FIELD_NUMBER: builtins.int
    MESSAGE_FIELD_NUMBER: builtins.int
    SOURCE_CODE_LOCATION_FIELD_NUMBER: builtins.int
    dataflow_graph_id: builtins.str
    """The graph being defined when the API usage occurred."""
    operation: builtins.str
    """The operation corresponding to the unsupported API usage. A value from the
    pipelines operation proto enum.
    """
    message: builtins.str
    """A human-readable message describing the usage."""
    @property
    def source_code_location(self) -> pyspark.sql.connect.proto.pipelines_pb2.SourceCodeLocation:
        """The source code location where the unsupported API was used."""
    def __init__(
        self,
        *,
        dataflow_graph_id: builtins.str | None = ...,
        operation: builtins.str | None = ...,
        message: builtins.str | None = ...,
        source_code_location: pyspark.sql.connect.proto.pipelines_pb2.SourceCodeLocation
        | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_dataflow_graph_id",
            b"_dataflow_graph_id",
            "_message",
            b"_message",
            "_operation",
            b"_operation",
            "_source_code_location",
            b"_source_code_location",
            "dataflow_graph_id",
            b"dataflow_graph_id",
            "message",
            b"message",
            "operation",
            b"operation",
            "source_code_location",
            b"source_code_location",
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_dataflow_graph_id",
            b"_dataflow_graph_id",
            "_message",
            b"_message",
            "_operation",
            b"_operation",
            "_source_code_location",
            b"_source_code_location",
            "dataflow_graph_id",
            b"dataflow_graph_id",
            "message",
            b"message",
            "operation",
            b"operation",
            "source_code_location",
            b"source_code_location",
        ],
    ) -> None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_dataflow_graph_id", b"_dataflow_graph_id"]
    ) -> typing_extensions.Literal["dataflow_graph_id"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_message", b"_message"]
    ) -> typing_extensions.Literal["message"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_operation", b"_operation"]
    ) -> typing_extensions.Literal["operation"] | None: ...
    @typing.overload
    def WhichOneof(
        self,
        oneof_group: typing_extensions.Literal["_source_code_location", b"_source_code_location"],
    ) -> typing_extensions.Literal["source_code_location"] | None: ...

global___UnsupportedAPIUsagePipelineCommand = UnsupportedAPIUsagePipelineCommand

class DatasetExpectation(google.protobuf.message.Message):
    """An expectation that the rows of a dataset satisfy a given invariant."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    NAME_FIELD_NUMBER: builtins.int
    INVARIANT_FIELD_NUMBER: builtins.int
    ACTION_FIELD_NUMBER: builtins.int
    DATASET_IDENTIFIER_FIELD_NUMBER: builtins.int
    SOURCE_CODE_LOCATION_FIELD_NUMBER: builtins.int
    name: builtins.str
    """The expectation name."""
    @property
    def invariant(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Boolean expression that defines the invariant."""
    action: global___ExpectationViolationAction.ValueType
    """What to do if the invariant is violated."""
    @property
    def dataset_identifier(self) -> pyspark.sql.connect.proto.common_pb2.ResolvedIdentifier:
        """The dataset to which this expectation applies."""
    @property
    def source_code_location(self) -> pyspark.sql.connect.proto.pipelines_pb2.SourceCodeLocation:
        """The source code location where the expectation is defined."""
    def __init__(
        self,
        *,
        name: builtins.str | None = ...,
        invariant: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
        action: global___ExpectationViolationAction.ValueType | None = ...,
        dataset_identifier: pyspark.sql.connect.proto.common_pb2.ResolvedIdentifier | None = ...,
        source_code_location: pyspark.sql.connect.proto.pipelines_pb2.SourceCodeLocation
        | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_action",
            b"_action",
            "_dataset_identifier",
            b"_dataset_identifier",
            "_invariant",
            b"_invariant",
            "_name",
            b"_name",
            "_source_code_location",
            b"_source_code_location",
            "action",
            b"action",
            "dataset_identifier",
            b"dataset_identifier",
            "invariant",
            b"invariant",
            "name",
            b"name",
            "source_code_location",
            b"source_code_location",
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_action",
            b"_action",
            "_dataset_identifier",
            b"_dataset_identifier",
            "_invariant",
            b"_invariant",
            "_name",
            b"_name",
            "_source_code_location",
            b"_source_code_location",
            "action",
            b"action",
            "dataset_identifier",
            b"dataset_identifier",
            "invariant",
            b"invariant",
            "name",
            b"name",
            "source_code_location",
            b"source_code_location",
        ],
    ) -> None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_action", b"_action"]
    ) -> typing_extensions.Literal["action"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_dataset_identifier", b"_dataset_identifier"]
    ) -> typing_extensions.Literal["dataset_identifier"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_invariant", b"_invariant"]
    ) -> typing_extensions.Literal["invariant"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_name", b"_name"]
    ) -> typing_extensions.Literal["name"] | None: ...
    @typing.overload
    def WhichOneof(
        self,
        oneof_group: typing_extensions.Literal["_source_code_location", b"_source_code_location"],
    ) -> typing_extensions.Literal["source_code_location"] | None: ...

global___DatasetExpectation = DatasetExpectation

class DefineDatasetExpectationsPipelineCommand(google.protobuf.message.Message):
    """Defines a set of expectations to be applied to datasets in a graph.
    Intended for use inside the extension field of PipelineCommand.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATAFLOW_GRAPH_ID_FIELD_NUMBER: builtins.int
    EXPECTATIONS_FIELD_NUMBER: builtins.int
    dataflow_graph_id: builtins.str
    @property
    def expectations(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        global___DatasetExpectation
    ]: ...
    def __init__(
        self,
        *,
        dataflow_graph_id: builtins.str | None = ...,
        expectations: collections.abc.Iterable[global___DatasetExpectation] | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_dataflow_graph_id", b"_dataflow_graph_id", "dataflow_graph_id", b"dataflow_graph_id"
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_dataflow_graph_id",
            b"_dataflow_graph_id",
            "dataflow_graph_id",
            b"dataflow_graph_id",
            "expectations",
            b"expectations",
        ],
    ) -> None: ...
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_dataflow_graph_id", b"_dataflow_graph_id"]
    ) -> typing_extensions.Literal["dataflow_graph_id"] | None: ...

global___DefineDatasetExpectationsPipelineCommand = DefineDatasetExpectationsPipelineCommand
